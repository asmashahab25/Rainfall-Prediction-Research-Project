{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZdIYYp-4-TY"
      },
      "outputs": [],
      "source": [
        "# Importing essential libraries for numerical operations and data handling\n",
        "import numpy as np                                                            # For numerical computations and array handling\n",
        "import pandas as pd                                                           # For data manipulation and analysis using DataFrames\n",
        "\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt                                               # For creating static, animated, and interactive plots\n",
        "import seaborn as sb                                                          # For advanced data visualization, built on top of matplotlib\n",
        "\n",
        "# Importing tools for data preprocessing and model evaluation\n",
        "from sklearn.model_selection import train_test_split                          # To split dataset into training and testing sets\n",
        "from sklearn.preprocessing import StandardScaler                              # To scale/normalize feature data\n",
        "from sklearn import metrics                                                   # For evaluating the performance of machine learning models\n",
        "\n",
        "# Importing machine learning models\n",
        "from sklearn.svm import SVC                                                   # Support Vector Classifier - good for classification problems\n",
        "from xgboost import XGBClassifier                                             # Extreme Gradient Boosting Classifier - powerful ensemble model\n",
        "from sklearn.linear_model import LogisticRegression                           # Logistic Regression - simple linear model for classification\n",
        "\n",
        "# Importing technique for handling imbalanced datasets\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Suppressing warning messages to keep output clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')                                             # Ignores warnings (e.g., from libraries or deprecated methods)\n",
        "\n",
        "# Loading the dataset from a CSV file named 'Rainfall.csv' into a pandas DataFrame\n",
        "df = pd.read_csv('Rainfall.csv')\n",
        "\n",
        "# Displaying the first 5 rows of the DataFrame to understand the structure of the data\n",
        "df.head()\n",
        "\n",
        "# Checking the number of rows and columns in the DataFrame\n",
        "df.shape\n",
        "\n",
        "# Displaying concise summary information about the DataFrame\n",
        "df.info()\n",
        "\n",
        "# Generating descriptive statistics for all numerical columns, and transposing the output for better readability\n",
        "df.describe().T\n",
        "\n",
        "#DATA CLEANING\n",
        "\n",
        "# Checking the total number of missing (null) values in each column\n",
        "df.isnull().sum()\n",
        "\n",
        "# Displaying a list of all column names in the DataFrame\n",
        "df.columns\n",
        "\n",
        "# Removing any leading or trailing whitespace from column names\n",
        "df.rename(str.strip,                                                          # Apply the str.strip() method to each column name\n",
        "          axis='columns',                                                     # Specify that we're renaming along the columns axis (not rows)\n",
        "          inplace=True)                                                       # Apply the changes directly to the original DataFrame without creating a new one\n",
        "\n",
        "# Displaying the cleaned column names\n",
        "df.columns\n",
        "\n",
        "# Looping through each column in the DataFrame\n",
        "for col in df.columns:\n",
        "\n",
        "  # Checking if the current column has any missing (null) values\n",
        "  if df[col].isnull().sum() > 0:\n",
        "\n",
        "    # Calculating the mean of the column (ignores NaN by default)\n",
        "    val = df[col].mean()\n",
        "\n",
        "    # Filling missing values in the column with the calculated mean\n",
        "    df[col] = df[col].fillna(val)\n",
        "\n",
        "# Checking the total number of missing values in the entire DataFrame after filling\n",
        "df.isnull().sum().sum()\n",
        "\n",
        "# Creating a pie chart to show the distribution of values in the 'rainfall' column\n",
        "plt.pie(df['rainfall'].value_counts().values,                                 # Sizes of the pie slices (counts of each unique value)\n",
        "        labels = df['rainfall'].value_counts().index,                         # Labels for each slice (the unique values in 'rainfall')\n",
        "        autopct='%1.1f%%')                                                    # Display percentages with one decimal place on each slice\n",
        "\n",
        "# Displaying the pie chart\n",
        "plt.show()\n",
        "\n",
        "# Creating a heatmap to visualize strong correlations between features\n",
        "plt.figure(figsize=(10,10))                                                   # Setting the size of the figure (10x10 inches)\n",
        "sb.heatmap(df.corr() > 0.8,                                                   # Computing correlation matrix and showing only those above 0.8 (strong correlation)\n",
        "           annot=True,                                                        # Annotating each cell with True/False\n",
        "           cbar=False)                                                        # Hiding the color bar (since we're only showing True/False)\n",
        "\n",
        "plt.show()                                                                    # Displaying the heatmap\n",
        "\n",
        "df.drop(['maxtemp', 'mintemp'], axis=1, inplace=True)                         # Dropping the 'maxtemp' and 'mintemp' columns from the DataFrame\n",
        "\n",
        "#MODEL TRAINING\n",
        "\n",
        "\n",
        "# Separating the input features (independent variables)\n",
        "# Dropping 'day' and 'rainfall' columns from df to keep only the predictor features\n",
        "features = df.drop(['day', 'rainfall'], axis=1)\n",
        "\n",
        "# Selecting the target variable (dependent variable) for prediction\n",
        "target = df.rainfall\n",
        "\n",
        "# Splitting the dataset into training and validation sets\n",
        "X_train, X_val, \\\n",
        "    Y_train, Y_val = train_test_split(features,                               # The independent variables (features)\n",
        "                                      target,                                 # The target variable (what we want to predict)\n",
        "                                      test_size=0.2,                          # 20% of the data will be used for validation/testing\n",
        "                                      stratify=target,                        # Ensures that class proportions are the same in both train and validation sets\n",
        "                                      random_state=2)                         # For reproducibility â€” ensures same split every time you run the code\n",
        "\n",
        "# As the data was highly imbalanced we will\n",
        "# balance it by adding repetitive rows of minority class.\n",
        "ros = RandomOverSampler(sampling_strategy='minority',                         # Oversample only the minority class to match the majority class count\n",
        "                        random_state=22)                                      # For reproducibility of sampling\n",
        "\n",
        "# Resampling the training data to balance the class distribution\n",
        "X, Y = ros.fit_resample(X_train, Y_train)\n",
        "\n",
        "# Normalizing the features for stable and fast training.\n",
        "\n",
        "scaler = StandardScaler()                                                     # Initializing the StandardScaler for feature normalization\n",
        "X = scaler.fit_transform(X)                                                   # Fitting the scaler on training data and transforming it\n",
        "X_val = scaler.transform(X_val)                                               # Using the same scaler to transform the validation data\n",
        "\n",
        "# Creating a list of classification models to train and compare\n",
        "models = [\n",
        "    LogisticRegression(),                                                     # A simple, interpretable linear model\n",
        "    XGBClassifier(),                                                          # A powerful gradient boosting model\n",
        "    SVC(kernel='rbf', probability=True)                                       # Support Vector Classifier with RBF kernel and probability outputs\n",
        "  ]\n",
        "\n",
        "# Looping through each model to train and evaluate using ROC AUC Score\n",
        "for i in range(3):\n",
        "\n",
        "  # Training the model on the balanced, normalized training set\n",
        "  models[i].fit(X, Y)\n",
        "\n",
        "  # Printing the model type\n",
        "  print(f'{models[i]} : ')\n",
        "\n",
        "  # Predicting probabilities on training set\n",
        "  train_preds = models[i].predict_proba(X)\n",
        "\n",
        "  # Evaluating training performance using ROC AUC Score\n",
        "  print('Training Accuracy : ', metrics.roc_auc_score(Y, train_preds[:,1]))\n",
        "\n",
        "  # Predicting probabilities on validation set\n",
        "  val_preds = models[i].predict_proba(X_val)\n",
        "\n",
        "  # Evaluating validation performance using ROC AUC Score\n",
        "  print('Validation Accuracy : ', metrics.roc_auc_score(Y_val, val_preds[:,1]))\n",
        "  print()                                                                     # Just for clean spacing between model outputs\n",
        "\n",
        "# Importing required libraries for plotting and evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "\n",
        "# Plotting the confusion matrix for the third model in the list (SVC)\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    models[2],                                                                # Using the trained SVC model (index 2 in the models list)\n",
        "    X_val,                                                                    # Validation features\n",
        "    Y_val)                                                                    # True validation labels\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n",
        "# Printing the classification report for the SVC model on validation data\n",
        "print(metrics.classification_report(\n",
        "    Y_val,                                                                    # True labels for the validation set\n",
        "    models[2].predict(X_val)                                                  # Predicted labels by the SVC model (index 2 in models list)\n",
        "    ))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}